---
description: 
globs: 
alwaysApply: true
---
# Tech Stack Document

The Hetri MVP platform spans from IoT hardware to cloud services to mobile apps. This document enumerates all major technologies selected for each part of the system (firmware, backend, video pipeline, messaging, frontend, etc.), along with the rationale behind each choice and any relevant version or configuration notes. The tech stack is chosen to balance rapid development with scalability, using modern, widely-supported tools that can evolve with the product.

## Smart Collar Firmware & Hardware

* **Hardware Platform:** **ESP32-S3 microcontroller** – chosen for its blend of performance and power efficiency. The ESP32-S3 has dual cores, integrated Wi-Fi, and support for camera input (DVP interface) which allows us to attach a camera module (like the 5MP **OV5640** sensor). It also has Bluetooth (for provisioning) and ample GPIOs for sensors and actuators. This chip can run FreeRTOS and has enough RAM to buffer images, making it suitable for a streaming camera use-case (albeit pushing its limits at higher resolutions). It's significantly lower power and cost than a full Linux board (like Raspberry Pi), aligning with our goal of a wearable device with long battery life.

* **Camera Module:** **OV5640** 5MP camera sensor – a common module compatible with ESP32 and similar. It can capture up to 1080p frames but the ESP32-S3's limitation requires using lower framerates or resolutions for streaming (e.g., 480p or 720p MJPEG due to lack of hardware H.264 encoder). The OV5640 is low-power and has built-in ISP functions for basic exposure control, making it a good fit for varied lighting (indoor/outdoor daycare environments).

* **Sensors:** **LIS3DH accelerometer** – a 3-axis accelerometer included for motion detection and activity monitoring. It's ultra-low-power and can wake the ESP32 from deep sleep on motion interrupts, which aids in power saving (only stream when needed). Potential to add a temperature sensor (some boards have it or can use the ESP32's internal temp sensor for device temperature). Future sensors like a microphone (for barking detection) or GPS (for outdoor tracking) can be integrated via I2C/SPI if needed, but MVP focuses on accelerometer and camera.

* **Firmware Language & Framework:** **C++ with ESP-IDF (Espressif IoT Development Framework)**. This is the official SDK for ESP32, providing low-level control and libraries for Wi-Fi, BT, camera driver, etc. We use C/C++ for fine control over hardware and performance. The firmware is essentially an embedded application: it connects to Wi-Fi, streams video, listens for MQTT commands, and monitors sensors. We leverage FreeRTOS (built into ESP-IDF) for multitasking (e.g., one task for video capture/stream, one for MQTT communication, one for sensor reading).

  * *Rationale:* ESP-IDF gives us stability and support for advanced features like TLS, and we can optimize at the hardware level. Alternatives considered: Arduino core for ESP32 (simpler but less control over advanced features), or switching to a different platform like Raspberry Pi Zero (which would allow using Python or higher-level but at cost of power and size). We chose ESP32-S3 for a more **integrated, IoT-centric approach** in MVP, with the understanding that video capabilities are somewhat constrained (we mitigate by server-side encoding).

* **Communication Protocols:**
  * **MQTT client (Embedded):** The firmware uses an MQTT library (Espressif's built-in MQTT or an open source one) to connect to the MQTT broker over TLS. This allows the collar to subscribe to command topics (like treat commands) and publish telemetry (sensor data, status updates). MQTT is lightweight and designed for low-bandwidth devices.
  * **Video Streaming:** The firmware will capture frames from the camera and transmit them. **MJPEG over RTMP or RTSP** is our approach: The device can capture JPEG frames and push them as a stream. One approach is using **RTSP (Real-Time Streaming Protocol)** – there are libraries that allow ESP32 to act as an RTSP server or client sending a video stream of JPEGs. Alternatively, the device could publish frames via HTTP to an ingest server. We choose **RTMP (Real-Time Messaging Protocol)** to send video to the cloud: the ESP32 will use a lightweight RTMP client implementation in C (some exist for embedded) to stream video frames to our cloud ingestion point. RTMP is chosen because our server stack (Nginx) can easily ingest RTMP and convert to HLS. WebRTC was considered (for truly low latency), but it's too heavy for ESP32 and complex for MVP. So, **RTMP+HLS** is a simpler, robust choice for one-way streaming.

* **Power Management:** Although not a "tech" in terms of stack, it's worth noting: we employ deep sleep and wake strategies (in code) to extend battery. The LIS3DH triggers wakes, and the ESP32 will use its ULP co-processor or timers for periodic wake-ups. This is implemented in firmware using ESP-IDF's power management APIs.

## Cloud Backend (Server Side)

* **Cloud Provider:** We will use **AWS (Amazon Web Services)** for hosting, taking advantage of services like EC2 (or ECS) for running our containers, **S3** for storing HLS video segments, and **CloudFront** CDN for delivering video globally. AWS IoT or others could be used for MQTT, but MVP likely uses self-managed broker for cost and simplicity. AWS is chosen for scalability and familiarity, but the architecture is cloud-agnostic enough to run on any VM or Kubernetes if needed.

* **Application Server:** **FastAPI (Python 3.x)** – this is the main backend web framework, implementing our REST API and core business logic. FastAPI is modern, high-performance (using Uvicorn/ASGI under the hood), and easy to write asynchronous code (important for handling many streaming connections or MQTT events). We choose Python/FastAPI because of rapid development speed, rich ecosystem (for database ORMs, JWT auth libraries, etc.), and our team's expertise. It also integrates well if we later incorporate machine learning (e.g., dog recognition, anomaly detection) or other data analysis.

  * The FastAPI app will define endpoints for login, treat requests, device pairing, etc. It also can include background tasks or event handlers for MQTT messages (using an async MQTT client subscribed to topics). Python's asyncio will let us handle incoming MQTT events and notify clients (perhaps via WebSockets) without blocking other requests.
  * *Version notes:* We'll target Python 3.10+ (for modern features), FastAPI latest (around 0.85+). Use Uvicorn as ASGI server, possibly with Gunicorn for production process management.

* **Alternative/Additional Service (Node.js):** While FastAPI is primary, the prompt mentioned "Express" as well. We might consider a **Node.js (Express.js) service** for certain tasks if needed, possibly for real-time communication. However, since FastAPI + WebSockets can cover it, we may not need Express in MVP. Another angle: Node could be used for a **web admin dashboard** if built server-side rendered or to serve static content. But likely unnecessary – our admin frontend will be in the app or a React web app (which can be served statically from S3/CloudFront or via FastAPI static files). So, we lean toward a single backend service (FastAPI) to reduce complexity. The mention of Express might be to highlight that our architecture is flexible; if we needed, for example, a dedicated notification service or an IoT gateway in Node, we could incorporate it. For now, we stick to Python for core logic and perhaps use Node only if needed for specific integrations (none identified for MVP).

* **Database:** **Microsoft SQL Server (MS SQL)** – the primary datastore for persistent data. We opt for MS SQL (running in a Docker container) possibly due to enterprise client integration or team familiarity. MS SQL provides robust relational storage, ACID transactions, and is scalable (can handle our expected load easily). It also supports features like JSON data types and full-text search if needed later. The choice might be influenced by compatibility with other systems the facilities use (some may have Windows environments or tools expecting ODBC).

  * We will use an **ORM** to interact with the database. In Python, **SQLAlchemy** (with its async support) is a good fit and can work with MS SQL via ODBC or pymssql drivers. The schema will be defined via ORM models. Alternatively, we could use Tortoise ORM or Peewee, but SQLAlchemy is industry-standard.
  * *Schema Notes:* We'll have tables for Users, Pets, Devices, TreatRequests, etc. The database will run as a Docker container in development; in production, we might use an AWS RDS SQL Server instance or Azure SQL. The stack is containerized so switching to Postgres or MySQL is possible, but we stick to MS SQL as given. We ensure to avoid using vendor-specific SQL so that migrations are easier.

* **MQTT Broker:** **Eclipse Mosquitto** – a lightweight open-source MQTT broker, which we'll containerize and run in the cloud (or use an AWS EC2 for it). Mosquitto supports MQTT v3.1.1 (and v5 if needed) and can handle many clients (our use case: one client per device, plus maybe one per admin app if we use MQTT for them too). It supports TLS for secure connections. We'll configure Mosquitto with appropriate security (allowing only authenticated connections, and using ACLs to restrict topics).

  * Rationale: We could use AWS IoT Core (a managed MQTT broker), but that might add complexity with provisioning certificates and cost per message. Mosquitto is simple and under our control. For MVP and initial scale (hundreds of devices), a single Mosquitto instance is fine. We'll enable persistence so it can retain session info and last-will messages (so we know if a device went offline unexpectedly).
  * The FastAPI backend will also connect to the MQTT broker (likely using an async MQTT client like `paho-mqtt` or `asyncio-mqtt`) to publish commands and perhaps to subscribe to device status topics. This way, the backend can react to events (e.g., a device published "treat delivered" status – the backend can then notify the owner via WebSocket).

* **Video Streaming Server:** **Nginx with the RTMP module**, plus **FFmpeg**. The video pipeline in cloud works as follows: The collar device streams video (via RTMP) to an Nginx server (with nginx-rtmp) running in a Docker container. Nginx-RTMP can directly output an HLS stream (it will take the incoming RTMP and segment it into .ts files and a .m3u8 playlist). We configure it to store these segments in an AWS S3 bucket, or serve them directly. To integrate with CloudFront CDN, the HLS output might be stored on S3 and CloudFront set to that bucket origin. Alternatively, CloudFront could pull from the Nginx server (if it has a public endpoint), but using S3 is more standard for HLS.

  * We use **FFmpeg** in conjunction for any needed transcoding. The ESP32 likely streams MJPEG (motion JPEG) or some low-compression format. To reduce bandwidth, an FFmpeg process can subscribe to the RTMP stream and transcode the video to H.264/AAC for HLS. Nginx's RTMP module can actually call ffmpeg internally via exec commands for transmuxing. We'll use FFmpeg to take the MJPEG frames and encode them to H.264 at, say, 720p 15fps (whatever the device can handle). This encoded stream is then fed back to Nginx which packages to HLS. This approach offloads heavy video encoding to the cloud where we have CPU/GPU resources, making the device's job easier.
  * *Rationale:* This solution is cost-effective and relatively straightforward. Alternatives: AWS Kinesis Video Streams or AWS MediaLive could handle ingest and transcoding but would be costlier and overkill for MVP. WebRTC as alternative would require a signaling server and is complex on device side. Thus, RTMP/HLS is a tried-and-true solution for one-way streaming to many viewers with tolerance for ~2s latency.
  * **CloudFront CDN:** We place CloudFront in front of the HLS content so that owners (who could be anywhere) get fast access with caching. CloudFront also provides the ability to secure streams via signed URLs. We will use CloudFront's signed URL feature or Nginx's secure link module to ensure only authorized viewers get the video. For MVP, a simple approach: the FastAPI backend generates a token and appends as query param to the HLS playlist URL, Nginx (or CloudFront Lambda@Edge) validates it. We likely use Nginx Secure Link module on our streaming server if not using CloudFront signed URLs.

* **WebSockets / Real-Time API:** We incorporate a WebSocket gateway in the backend for real-time events to the apps. FastAPI can serve WebSocket endpoints (for example, the mobile app can open a WebSocket after login to listen for events like treat confirmations or help alerts). This is simpler than polling and complements MQTT which is device-focused. The WebSocket will be authenticated (perhaps with a JWT when connecting). Then the backend can push notifications: e.g., when a treat is marked delivered, backend sends a message to the owner's app; when a device's battery is low, it sends a message to the admin's app. We use **Starlette/WebSocket** support of FastAPI. If scaling becomes an issue, we could integrate Redis or a pub/sub for multiple server nodes to broadcast events. MVP likely runs on one server process, so not a big issue.

  * *Why not use MQTT for app too:* We could have the mobile app subscribe to MQTT for treat status or alerts. But that would require embedding an MQTT client in the app, maintaining a separate connection, etc. Using WebSocket over HTTPS is more straightforward for apps (one less protocol for the app developers to deal with). The backend acts as the bridge – it receives device MQTT messages, then forwards relevant info to app via WebSocket. This pattern is common in IoT backends.

* **Auth & Security:** We use **JWT (JSON Web Tokens)** for authenticating API requests. On login, the FastAPI backend issues a JWT signed with a secret, containing the user's ID, role, and tenant ID (and an expiry). The mobile app stores this and sends it in the `Authorization` header for subsequent requests. We also use refresh tokens or short expiry with refresh endpoint for security (MVP can keep it simple with a reasonably long-lived token that can be revoked server-side via a blacklist if needed). For device authentication to MQTT, we might use username/password where username is device ID and password is a secret or token issued at pairing time. Mosquitto can be configured with a password file or an authentication plugin – possibly we integrate JWT or a simple database lookup for device credentials. Since devices are constrained, using JWT might be heavy; instead, generate each device a unique username & password or client certificate. For MVP, a username/password (with TLS) is acceptable for MQTT.

## Frontend (Mobile/Web App)

* **Framework:** **React Native** – for a unified mobile application that can target both iOS and Android with one codebase. This is chosen to speed development and ensure consistent features for all users. The RN app will serve both **PAW Parents** and **PAW Admins**; we will implement role-based UI so that after login, the app determines which set of screens to show. React Native allows using rich libraries for UI components and can integrate native modules if needed (like a video player for HLS, or perhaps QR scanner for device pairing).

  * We will use **TypeScript** with React Native for type-safety and better developer experience. This helps catch errors early and serves as documentation for the shapes of our data (especially important when interacting with our API responses).

* **State Management:** We plan to use a combination of **React Context** for simple global states (like the logged-in user info, theme) and **Redux (Redux Toolkit)** for more complex state like lists of pets, treat request queue, etc. Redux Toolkit's slices can make it easier to manage and update global state in an organized way. Alternatively, for a simpler approach, we might use React's built-in context and hooks for most things and avoid the boilerplate of Redux in MVP. But given multiple user roles and possibly real-time updates, Redux with something like Redux-Thunk or Redux-Saga could help manage asynchronous actions and ensure different parts of the app stay in sync. We choose Redux Toolkit for a structured approach, while acknowledging that for very simple flows context might suffice – we opt for scalability with Redux from the start, to handle things like a realtime feed of notifications, etc.

* **UI Library & Design:** We will use a UI component library such as **React Native Paper** or **Native Base** to get a set of pre-styled components (buttons, cards, etc.) that work cross-platform. This speeds up UI development and ensures consistency. Our design will follow a clean, modern look with the facility's branding where appropriate. We'll also enforce certain UX principles: e.g., the **Von Restorff effect** for key actions – the TreatTap button will be a distinct color (perhaps the only bright accent on screen) to draw attention, ensuring users easily find it. We also apply standard mobile UX guidelines like reachable tap targets (important for staff possibly using tablets with one hand while wrangling a dog!).

* **Navigation:** Using **React Navigation** (likely the latest v6) for managing screens and flows. We'll have at least two navigation stacks: one for Pet Parent (with screens like Home/LiveView, maybe Treat history, Profile) and one for Admin (with screens like Dashboard of Dogs, Live camera view, Treat Requests list, Device management, etc.). On app startup, we decide which stack to load based on user role. We might use a bottom tab navigator or drawer for some sections (e.g., an admin might have tabs for "Dogs", "Devices", "Settings"; an owner might have tabs for "Live", "Activity Log", "Account"). React Navigation provides stack navigators for flows like login → main, and tab navigators for organizing top-level sections.

* **Video Playback:** We will need a video player that can play HLS streams. On iOS, HLS is natively supported in the system player. On Android, we can use an library like **react-native-video** which supports HLS via ExoPlayer. We'll integrate this so that the Live Stream screen can display the HLS feed given the URL. We might enable adaptive bitrate if multiple qualities are offered (not likely in MVP). A potential challenge is authentication for the HLS – since we use signed URLs, the player just needs to fetch it normally. If we required cookies or headers, we'd need to ensure the player can be given those (react-native-video allows headers to be passed if needed).

* **QR Code Scanning:** For device pairing and possibly owner invite QR, we will use **react-native-camera** or the newer expo-camera (if using Expo) to scan QR codes. There are libraries like `react-native-qrcode-scanner` that simplify this. The camera permission and handling will be needed in the staff app context (scanning a collar code).

* **Real-Time Updates:** The app will maintain a **WebSocket** connection to receive push events. In React Native, we can use the built-in WebSocket API to connect to our backend's WS endpoint. We'll likely establish this after login and keep it open. We'll define a protocol for messages (e.g., JSON with a type field: `{"type": "treat_confirm", "petId":..., "timestamp":...}`). The app's Redux store or context can have an event handler that updates state based on these messages (for example, if `type=="treat_request" and user is staff, increment the pending treat counter). This way, admins see new treat requests in real time, and owners get treat delivered notifications without polling.

* **CI/CD for App:** We include this in tech stack as well: we plan to use **Expo EAS (Expo Application Services)** or a combination of **fastlane** and **GitHub Actions** to automate building the app for iOS and Android. If using Expo (which can be in bare or managed workflow), it can simplify OTA updates for minor fixes. However, since we might need custom native modules (for video or camera), we may eject to bare RN. In that case, we'll set up a pipeline: e.g., GitHub Actions triggers on new main branch commit, runs tests, then uses fastlane to build APK/IPA and possibly deploy to TestFlight/Play Store for beta. For MVP internal testing, we can use something like TestFlight and Google Play Internal Test track.

## DevOps and CI/CD

* **Source Control:** All code (firmware, backend, frontend) will be managed in Git (e.g., GitHub or GitLab). We'll have separate repositories or a monorepo depending on team structure. Possibly a monorepo with sub-folders for firmware, backend, and app for easier coordination – but using separate repos is fine too. We will enforce code reviews and use feature branching to integrate changes.

* **Continuous Integration:** Set up CI pipelines using **GitHub Actions** (or GitLab CI). Each push triggers linting and tests for the respective part. For the backend (Python), run unit tests (pytest) and check code style (flake8, black). For the app, run ESLint/TypeScript checks and perhaps unit tests for any logic (though RN UI tests might be minimal). For firmware, maybe run static code analysis (like ESP-IDF's `idf.py analyze` or at least ensure it compiles for target).

* **Continuous Deployment:** We will use a **Docker-based deployment** for the backend. A Docker Compose configuration will define all services (FastAPI, MS SQL, Mosquitto, Nginx, etc.). For production, we might deploy on an EC2 instance: basically pull the latest images and compose up. In CI, we can build Docker images on each release. For example, build `hetri-backend:latest` and push to a container registry. The same for Nginx-RTMP if we have a custom image with our config. Then on the server, a deploy script pulls the new images and restarts containers with minimal downtime. We'll incorporate Let's Encrypt (Certbot) for certificates, possibly as a cron or a small container that runs and updates Nginx certs.

* **Monitoring & Logging:** We plan to include basic monitoring – likely use **AWS CloudWatch** or simply docker logs to track issues. We'll ensure the backend logs important events (user logins, treat requests, errors) with timestamps. For future, integrating something like **Prometheus/Grafana** for metrics (like number of active streams, CPU usage) would help, but MVP might just use built-in cloud monitors. Error tracking for the app (like Sentry) could be included to catch app crashes early from user devices.

Overall, the selected tech stack is meant to leverage proven solutions: an IoT-friendly microcontroller, a scalable real-time backend with FastAPI and MQTT, standard HLS streaming for video (taking advantage of widespread support), and cross-platform mobile development via React Native. This allows the small team to iterate quickly and deliver the core experience without reinventing the wheel, focusing on integrating these technologies into one cohesive platform.



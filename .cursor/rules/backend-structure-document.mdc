---
description: 
globs: 
alwaysApply: true
---
# Backend Structure Document

This document describes the architecture and structure of the Hetri backend system, including the division of services, directory layouts, data models, authentication, and how we support multiple tenant (facility) environments. Following this structure will ensure that the system is organized, secure, and scalable.

## Services and Responsibilities

The backend is composed of several distinct services/modules, each with clear responsibility:

* **API Service (FastAPI Application):** This is the primary web service that handles HTTP requests from clients (mobile app or web admin). It provides RESTful endpoints for all core operations: user authentication, pet info retrieval, treat request submission, device pairing, etc. It encapsulates business logic and acts as an integration point between other components (database, MQTT, streaming). For example, when an owner submits a treat request via the app, the API service authenticates the call, writes a log to DB, then publishes a message to the MQTT broker for the device. It also subscribes to certain MQTT topics (like device status) to update the DB or trigger further actions. This service also hosts any WebSocket endpoints for push notifications.

* **MQTT Broker (Mosquitto):** The MQTT broker handles real-time messaging to and from IoT devices (collar cameras). It's not custom code we write, but we run and configure it as part of our infrastructure. The broker receives **commands** from the API service (e.g., "dispense treat" to a specific device topic) and delivers them to the device, and it receives **telemetry** from devices (e.g., "battery=20%" on a telemetry topic) and routes them to subscribers (the API service might subscribe to telemetry topics, or we use the broker's retained messages feature for latest values). The broker is configured with TLS and authentication, and topic-based access control so devices and apps only interact with allowed topics.

* **Streaming Server (Nginx+RTMP+FFmpeg):** This service is dedicated to video streaming. Nginx with the RTMP module listens for incoming RTMP streams from devices. FFmpeg (possibly launched as a sidecar process or via Nginx exec) transcodes the stream as needed (MJPEG to H.264) and Nginx outputs HLS segments. This service is mostly static configuration and does not change per request – it continuously serves video segments. The API service might communicate with it indirectly (e.g., API might generate a secure token that Nginx's secure_link module uses to validate a playlist request).

* **Database (MS SQL):** The relational database stores persistent data: users, pets, devices, treat logs, etc. It enforces data integrity and relationships. We run it as a container (for development) or use a managed instance in production. The API service interacts with the DB via SQLAlchemy ORM or direct SQL for specific queries. For example, when a device pairs, API inserts a new Device row or updates an existing one to mark it paired to a tenant/pet. When a treat request happens, API logs a TreatRequest entry with status. The DB is authoritative for most data aside from live video, which is ephemeral.

* **Auth & Identity Module:** Part of API service but worth noting separately. This manages user accounts, hashing passwords, issuing JWT tokens, and validating permissions. It might include an integration with third-party OAuth in future (but MVP likely uses our own username/password system). This module ensures that each request is authenticated and has the right role/tenant for the operation (authorization checks).

* **Admin/Support Tools:** This could be an additional interface or scripts for Hetri Admins (internal). For example, a script to add a new Tenant and initial user could be present, or a small web UI behind admin login to monitor tenants. MVP might simply rely on direct DB access or minimal endpoints for these actions, but in structure we keep the possibility of an "admin" service or endpoints that are restricted to Hetri staff (with appropriate auth checks).

All these services are containerized and orchestrated (using Docker Compose in development, possibly docker swarm or Kubernetes in production later). They communicate primarily over network interfaces: API to DB (over SQL/TDS port), API to MQTT (over MQTT port), API to streaming (not much direct calls, except via generating URLs), devices to MQTT, devices to streaming, and clients to API and streaming.

## Directory and Repository Layout

We maintain a clean project structure in the repository for the backend code (FastAPI and related config):

```
backend/
├── app/
│   ├── main.py            # FastAPI application instance and startup logic
│   ├── api/               # API route definitions
│   │   ├── auth.py        # authentication routes (login, register if applicable)
│   │   ├── pets.py        # routes for pet info, maybe GET /pets/:id etc.
│   │   ├── treats.py      # routes for treat requests
│   │   ├── devices.py     # routes for device pairing, status
│   │   └── admin.py       # routes for admin actions (if any, like adding users)
│   ├── models/            # SQLAlchemy models (could be single file or multiple)
│   │   ├── __init__.py    # import of all models for metadata
│   │   ├── user.py
│   │   ├── pet.py
│   │   ├── device.py
│   │   ├── treat_request.py
│   │   └── ... (other models)
│   ├── schemas/           # Pydantic models for request/response bodies
│   │   ├── auth_schema.py
│   │   ├── pet_schema.py
│   │   └── ...
│   ├── services/          # Core business logic, if separated from routes
│   │   ├── auth_service.py  # functions for login, token gen
│   │   ├── treat_service.py # maybe a function to handle treat request logic
│   │   └── ...
│   ├── mqtt/              # MQTT client and handlers
│   │   ├── client.py      # code to connect to broker and subscribe
│   │   └── handlers.py    # functions that handle incoming MQTT messages (e.g., update DB on telemetry)
│   ├── utils/             # utility functions (e.g., JWT utility, time formatting, etc.)
│   ├── core/              # core config and setup (database connection, JWT config, etc.)
│   │   ├── config.py      # reading environment variables (DB connection string, secrets)
│   │   ├── db.py          # sessionmaker or async engine setup for DB
│   │   ├── security.py    # password hashing, JWT encode/decode functions
│   │   └── mqtt_config.py # MQTT connection details, topics definitions
│   └── main_config.yaml   # maybe config for logging, etc.
├── tests/
│   ├── test_auth.py
│   ├── test_treat_flow.py
│   └── ...
├── Dockerfile             # container build for FastAPI app
└── docker-compose.yml     # to tie together app, db, mqtt, etc. in dev
```

Additionally, outside `backend/`, we have separate directories or repos for:

* `mqtt/` (if we keep custom config for Mosquitto like acl file or conf file).
* `streaming/` (Nginx configuration and FFmpeg scripts, possibly as its own Docker context).
* Infrastructure as code (optional, e.g., terraform or AWS CloudFormation scripts if used to create cloud resources).

Within the `app/api` routes, we use **FastAPI's router** to structure by feature (as shown). Each route function will typically:

* verify the request JWT (FastAPI dependencies for auth, e.g., `get_current_user` dependency that checks token and returns user or raises 401),
* perform the necessary logic or call a service function,
* commit any DB changes via a session,
* and return a Pydantic schema response.

We maintain **separation of models and schemas**: SQLAlchemy models represent DB tables, Pydantic schemas define the shape of input/output data. This prevents accidental leakage of internal fields and decouples the API format from the DB structure.

## Authentication and Authorization Model

* **Authentication:** We use JWT tokens. The backend provides a `/auth/login` endpoint where user provides email/username and password. We verify against the hashed password in DB (using a strong hashing algorithm like bcrypt). If valid, we create a JWT token that includes:
  * `sub` claim as user ID,
  * `role` claim (e.g., "parent", "staff", "admin"),
  * `tenant_id` claim (the facility the user belongs to, for staff/parents; for Hetri Admins, this could be null or a special value meaning "all"),
  * an expiration (e.g., 1 hour).
  
  We sign it with our server's secret key. The token is returned to the client and stored (in mobile app secure storage). For subsequent requests, clients send `Authorization: Bearer <token>`. The FastAPI dependency will decode token, ensure it's not expired, and retrieve the user info (possibly hitting DB for fresh user or storing some in token directly).

  * Device Authentication: Devices authenticate to MQTT broker using credentials, not JWT (since JWT might be too short-lived and heavy for device to refresh). Instead, during device pairing, we generate an MQTT username and password for the device (or use the device's ID and a pre-shared secret as password). The Mosquitto broker will have these credentials (either in a password file or via a plugin that checks against our DB). We might also use MQTT's ability to require a client certificate – but that's more complex to set up for MVP. A simple solution: device connects with `clientId = device:<deviceId>` and `username = deviceId`, `password = someToken`. The API, after pairing, provides that password to the device (maybe via the pairing response). Mosquitto is configured with an ACL file mapping that deviceId to allowed topics.

* **Authorization:** We enforce role-based and tenant-based access in every route:
  * For example, the `/treat` endpoint requires that the user is a Pet Owner and that the pet requested belongs to them. Implementation: when owner calls POST /pets/{id}/treat, the `get_current_user` dependency provides user with `role='parent'` and user.id, we then query or check that pet.id belongs to user.id (we might have a Pet.owner_id linking to user). If not, return 403 Forbidden.
  * For a staff accessing an endpoint like GET /pets in their facility, we ensure `role='staff'` (or admin) and then filter results by `pet.tenant_id == user.tenant_id`. The JWT's tenant claim is crucial here – it tells us which facility's data they can operate on. We use that in DB queries either implicitly (if our ORM query always includes filter) or explicitly for each relevant query.
  * Hetri Admin (role 'superadmin' perhaps) can bypass tenant restrictions if needed but we'll still explicitly allow those in code for certain endpoints. Possibly via a dependency like `get_current_admin` that ensures the role is admin.

  * **Multi-Tenant Data Isolation:** In the database schema, nearly every table that is tenant-specific will have a `tenant_id` column. For example:
    * Users table has tenant_id (nullable or blank for Hetri admins who are not tied to one).
    * Pets table has tenant_id.
    * Devices table has tenant_id (which facility currently owns that device).
    * TreatRequests table has tenant_id (to easily query all requests for a facility, though it could derive from pet->tenant join, but storing for convenience and faster queries).
    
    We will likely use foreign keys to a Tenants table (Tenant has id, name, etc.), and use that to enforce referential integrity. For example, Pet.tenant_id references Tenants.id; Pet.owner_id references Users.id (and ideally that user's tenant_id must match Pet.tenant_id – this kind of constraint we might enforce at application level or via triggers because SQL foreign key across tenant might not directly ensure both have same tenant). At minimum, our service logic ensures you cannot mix data across tenants (we never assign a pet to a user of a different facility).

  * **Access Control on MQTT Topics:** The broker's ACL will ensure devices can only subscribe/publish to their own topics, and similarly, if the admin app were to connect to MQTT (we probably don't do that for app, using WebSocket instead), we'd ensure separate credentials. Assuming only devices use MQTT: each device `deviceId` can subscribe to `devices/<deviceId>/#` (all subtopics for itself) and publish to `devices/<deviceId>/#`. It cannot listen to other device topics. The backend (FastAPI service) might have a super client that can publish to any device topic and subscribe to e.g. `devices/+/status` (plus is wildcard) to catch all status messages. That MQTT client (in backend) is like an admin with full rights, and we protect its connection with strong credentials as well (or run it as localhost connection without exposing outside).

## Database Schema Design (MS SQL)

We design the DB with clarity and relationships in mind. Key tables:

* **Tenants:** (`tenant_id` PK, name, contact_info, etc.). Each facility is one row. Could also have columns like max_devices_allowed if needed for licensing.

* **Users:** (`user_id` PK, name, email, password_hash, role, tenant_id FK -> Tenants, etc.). Index on email (for login). For role, use an ENUM or a small int (1=owner, 2=staff, 9=hetriAdmin, etc.). If a user (like an owner) could belong to multiple facilities, we'd need a separate join table, but MVP likely one facility per user for simplicity. Hetri Admin users might have tenant_id null or a special value.

* **Pets:** (`pet_id` PK, name, breed, owner_id FK -> Users, tenant_id FK -> Tenants, etc.). Store info about each pet. A pet belongs to one owner (assuming one primary owner account) and one tenant (the facility they primarily go to). If multi-facility usage for a pet, that's complex; MVP assume pets tied to one facility (they typically register at one daycare). Possibly store things like birthdate, notes, but not needed for MVP beyond linking to owner and maybe breed for interest.

* **Devices:** (`device_id` PK, hw_serial, tenant_id FK -> Tenants, paired boolean, pairing_key, status, last_online, firmware_version, etc.). This stores the collar info. `device_id` might be some unique string (like MAC or a generated GUID). `pairing_key` is the secret used for pairing (set at manufacturing or generated and stored until used). Also a `current_pet_id` maybe, if we want to track which pet currently wearing it (this can change daily). Alternatively, we have a separate table for assignments (see below). But a quick way is to have Devices.current_pet_id that staff updates on assignment, and null when free. We also log `last_online` timestamp (updated when device pings or disconnects) for monitoring.

* **PetDeviceAssignment (optional):** If we want to log historical usage, a table with (assignment_id, pet_id, device_id, start_time, end_time). Each day a dog is checked in with a device creates a record, and end_time set at check-out. This helps historically see which device was on which pet and when. It's not strictly needed for operations, but useful for analytics or if something goes wrong ("which device did Bella have last Monday?"). For MVP we might not implement this fully, but the schema is considered. Staff's assignment action would create or update this table.

* **TreatRequests:** (`request_id` PK, pet_id FK, owner_id FK, timestamp, status, fulfilled_time, fulfilled_by (staff user id), maybe a message). Each treat tap from an owner generates a row. `status` could be "PENDING", "COMPLETED", "CANCELED". For MVP, maybe it's always either completed or canceled, since staff will act on it quickly. We keep these logs so owners potentially could see how many treats they gave, and facilities have a record (for possibly charging or just oversight). This table is definitely tenant-scoped via the pet -> tenant relationship, but we might denormalize and store tenant_id to easily query "all treat requests today for tenant 5".

* **ActivityLogs / Telemetry:** Possibly a table for periodic data from devices. For example, `DeviceTelemetry` (id, device_id, timestamp, battery_level, motion_level, temp, etc.). However, storing every single telemetry ping might be too granular. Instead, maybe store significant events:
  * **BatteryEvents:** when battery crosses certain thresholds (e.g., <20%), log an event (device_id, timestamp, level).
  * **MotionEvents:** if we detect meaningful events like "dog started moving after X minutes idle" or "no motion for Y minutes", those could be logged.
  
  This could be overkill for MVP. Alternatively, we might not store telemetry historically at all in MVP; just keep latest values in memory or broker (e.g., broker retains last known battery in a topic). But since we have SQL, we might store at least last known in Device table fields. E.g., Device.battery_level updated whenever a new value comes. And maybe Device.last_active_time updated when motion is detected. So staff can see current status, and not worry about historical graph now.

* **HelpAlerts:** If implementing help alerts formally, a table (alert_id, device_id or pet_id, timestamp, type ("HELP", or "SOS"), resolved_by, resolved_time, etc.). Or treat it as part of a generic Event table that could also include treat events. But likely separate for clarity.

* **Sessions/Tokens:** We might not need a DB table for sessions if using JWT statelessly. If we wanted to be able to revoke tokens, we might store a token ID in DB with expiration. MVP likely doesn't, relying on short expiry and secret rotation if needed.

* **Other tables:** If implementing parts of PAW management like bookings or staff schedules, those would have their own tables, but that's beyond MVP core scope.

**Note on MS SQL specifics:** We'll use appropriate data types (e.g., `VARCHAR(255)` for names, `UNIQUEIDENTIFIER` for IDs if using GUIDs, `DATETIME2` for timestamps or `DATETIMEOFFSET` if storing timezone). We must be mindful of quoting and reserved words in T-SQL when writing queries via SQLAlchemy. Also, ensure that foreign key constraints are added to maintain referential integrity between these tables (so we can't have a pet with owner that doesn't exist, etc.). Use cascading deletes carefully (maybe don't delete Pets or Users, usually mark inactive to preserve history).

## MQTT Topics Structure

To organize device communication, we define a clear topic hierarchy in the MQTT broker:

We use the convention including tenant and device for isolation, something like:

```
hetri/{tenantId}/devices/{deviceId}/...
```

However, since the broker will enforce via credentials which topics each device can use, we might simplify to:

```
devices/{deviceId}/...
```

with the knowledge that device IDs themselves are unique across tenants (we can ensure they are globally unique, e.g., prefix with tenant or random GUIDs). For clarity, we might incorporate tenant in topic anyway, as it makes debugging easier and double-protects isolation.

**Topics:**

* **Command Topics (downlink to device):** `devices/{deviceId}/cmd/{commandName}`. For example:
  * `devices/device123/cmd/treat` – the backend publishes here to tell that specific device to perform the treat action (could be to play a sound, etc.). The message payload might be JSON or just a simple string like "NOW" or an incrementing request ID. We might include an ID so device can optionally confirm which request it completed.
  * We could also have other commands in future, like `devices/abc/cmd/led_on` or `.../cmd/restart`. But MVP primarily needs `treat` command. Possibly a `sleep` command if we want to remotely put device to sleep, but that might be automatic.

* **Status/Telemetry Topics (uplink from device):** `devices/{deviceId}/status/{statusType}`. Examples:
  * `devices/device123/status/battery` – device publishes maybe `{"level": 18}` (meaning 18%) periodically or when low.
  * `devices/device123/status/telemetry` – could be a combined payload for multiple sensor data (e.g., `{"activity": "running", "temp": 22.5}`).
  * `devices/device123/status/treat` – if device confirms treat action done (like if it had a dispenser or just to log that it beeped for treat). This could also be piggybacked on a generic status or event topic.
  * `devices/device123/status/alert` – if device triggers a help alert or any alert, publish here, e.g., payload `{"type": "HELP", "message": "button pressed"}`.

* **Stream Topics:** We are not sending video through MQTT (too heavy). Instead, the device streams to RTMP. So no video data in MQTT. If we were to do snapshots, we could have `devices/123/data/snapshot` with a base64 image in payload, but not needed given our video pipeline.

* **Will (Last Will Testament):** Each device's MQTT client will be set with a Last Will on something like `devices/{deviceId}/status/online` = `false`. So that if a device goes offline ungracefully, the broker will publish that to indicate device offline. We can set `online` status messages:
  * Device connects: publish `devices/123/status/online` = `true`.
  * Device disconnects or crashes, broker publishes will: `.../online` = `false`.
  
  The API service can subscribe to `+/status/online` for all and update DB accordingly (Device.last_online and maybe a flag). Or at least, when owner tries to view and device offline, we know why.

**ACL considerations:** If using `devices/{deviceId}/...` topics, each device's auth will only allow subscribe to `devices/device123/cmd/#` (commands directed at it) and publish to `devices/device123/status/#`. It should not subscribe or publish elsewhere. The backend service (we might create an MQTT client that runs with admin privileges) can subscribe to e.g. `devices/+/status/#` to catch all status messages (the `+` wildcard for one level, `#` for multi-level). Or we might subscribe specifically to certain topics of interest like `+/status/alert` and `+/status/treat`. For publishing, the backend needs to publish to any device's cmd, so it needs broad publish rights (could subscribe too if it wanted to see its own messages). We'll give the backend's MQTT connection super-user privileges or at least ACL to publish to `devices/+/cmd/#`. The admin staff app is not connecting to MQTT in our design (it gets events via WebSocket from backend), so no need for MQTT ACL for staff/owners. But if we ever allowed staff to connect (some IoT apps let clients subscribe to device topics directly), we'd similarly have an ACL rule (e.g., staff of tenant can subscribe to that tenant's topics). We avoid that complexity for now.

**MQTT QoS:** Use QoS 1 (at least once) for commands and important messages like treat requests, to ensure delivery. QoS 0 (at most once) could be used for frequent telemetry that we don't mind losing occasionally (like motion data every few seconds – if one drops, next will come). But battery status and treat command should be QoS 1 so that it's acknowledged. The device and broker should use persistent sessions if possible, and we might allow retained messages for last-known state topics (like a retained `battery` level so that when an admin app or backend connects it can immediately get last value). However, since our backend is always connected to broker (ideally), it will maintain state.

## Multi-Tenant Support Notes

Supporting multiple facilities (tenants) is integral:

* **Data Partitioning:** As discussed, every relevant database query should filter by tenant. To enforce this, at the code level we might use middleware or utilities. For example, once a user is authenticated, we know `current_user.tenant_id`. For any query that fetches, say, pets, we ensure to include `.filter(Pet.tenant_id == current_user.tenant_id)`. We could build a BaseModel class that automatically adds tenant filter if the model has tenant_id – but that's tricky to generalize. More straightforward: be vigilant in each repository function or query to always filter. Also ensure on creation of objects, assign the tenant_id from the current context. E.g., when creating a TreatRequest, set `tenant_id = current_user.tenant_id` (redundant if we also set pet, but it's okay).

* **No Cross-Tenant IDs:** The API should never accept an ID from one tenant that could reference another tenant's object. For example, user could not craft a treat request for a pet ID that isn't theirs because we'll check the pet belongs to their tenant. All IDs can be global primary keys, but we ensure via checks that one tenant can't access another's. Even if an owner somehow found another facility's pet ID, the backend check prevents it.

* **Tenant-specific Config:** Some behavior may vary per tenant (though MVP features are uniform). However, things like treat cooldown or working hours might differ. So, we might have a `TenantSettings` table or include columns in Tenants (e.g., treat_cooldown_minutes, business_hours). The backend would then consult those for relevant operations. For example, before allowing a treat request, check if current time is within that tenant's hours of operation. Or if one facility disallows TreatTap for certain times, enforce it. This ensures our platform can serve different facility policies. MVP can have defaults and not expose too many custom settings yet, but structure is ready.

* **Scaling Consideration:** Initially, all tenants share the same DB and services. Our queries are always filtered, but as number of tenants grows, we need to ensure no one tenant's heavy usage starves others. The database can handle many tenants as long as indices are in place (e.g., indexing `tenant_id` on large tables, and perhaps composite index with other fields for common query patterns). We might consider sharding by tenant in future (each tenant in separate schema or database), but MVP sticks to one database for simplicity.

* **Tenant Isolation in Memory:** The backend service will inevitably have caches or in-memory data. We must ensure if we cache something like "list of pets" we key it by tenant. Using per-tenant caching to avoid mixing data. Similarly, if using global structures (like a dictionary of device_id -> current status), include tenant if not globally unique. But since device IDs are globally unique, maybe not needed there. Nonetheless, be cautious.

* **Testing Multi-tenancy:** We will test scenarios where two tenants have overlapping identifier spaces (like Pet ID 1 in tenant A and Pet ID 1 in tenant B) to ensure one cannot access the other. Also test that actions in one tenant do not appear in another (like staff of A doesn't receive treat events from B). This is mostly guaranteed by our checks and how topics are separate (device IDs unique), but careful testing is vital.

By following this backend structure and design, we ensure that the MVP is robust: the codebase is organized by responsibility, the database is structured for integrity and performance, real-time communications are well-defined, and multi-tenant isolation is enforced. This sets a strong foundation for future development, making it easier to add new features (like more sensor data or analytics) without restructuring everything.


